{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ nanochat Backward/Bidirectional Language Models\n",
    "\n",
    "This notebook lets you run inference with nanochat models trained in different directions:\n",
    "\n",
    "| Model | Direction | Description |\n",
    "|-------|-----------|-------------|\n",
    "| `nanochat-760M-backward` | Backward | Right-to-left prediction |\n",
    "| `nanochat-760M-bidirectional` | Bidirectional | Both directions with special tokens |\n",
    "| `nanochat-760M-backward-sft` | Backward | Chat fine-tuned backward model |\n",
    "\n",
    "**Repository:** [github.com/traghav/onanchat](https://github.com/traghav/onanchat)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the onanchat repository for model code\n",
    "!git clone https://github.com/traghav/onanchat.git\n",
    "%cd onanchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Add to path\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select and Download Model\n",
    "\n",
    "Choose which model to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select Model\n",
    "MODEL_CHOICE = \"nanochat-760M-backward\" #@param [\"nanochat-760M-backward\", \"nanochat-760M-bidirectional\", \"nanochat-760M-backward-sft\"]\n",
    "\n",
    "REPO_ID = f\"raghavt/{MODEL_CHOICE}\"\n",
    "print(f\"Selected model: {REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files from HuggingFace\n",
    "print(\"Downloading model files...\")\n",
    "\n",
    "model_path = hf_hub_download(repo_id=REPO_ID, filename=\"model.pt\")\n",
    "meta_path = hf_hub_download(repo_id=REPO_ID, filename=\"meta.json\")\n",
    "tokenizer_path = hf_hub_download(repo_id=REPO_ID, filename=\"tokenizer.pkl\")\n",
    "token_bytes_path = hf_hub_download(repo_id=REPO_ID, filename=\"token_bytes.pt\")\n",
    "\n",
    "print(f\"âœ“ Model downloaded to: {model_path}\")\n",
    "print(f\"âœ“ Metadata downloaded to: {meta_path}\")\n",
    "print(f\"âœ“ Tokenizer downloaded to: {tokenizer_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nanochat.gpt import GPT, GPTConfig\n",
    "\n",
    "# Load metadata\n",
    "with open(meta_path, 'r') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "direction = meta.get('direction', 'forward')\n",
    "model_config = meta['model_config']\n",
    "\n",
    "print(f\"Model direction: {direction}\")\n",
    "print(f\"Model config: {json.dumps(model_config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "config = GPTConfig(\n",
    "    sequence_len=model_config['sequence_len'],\n",
    "    vocab_size=model_config['vocab_size'],\n",
    "    n_layer=model_config['n_layer'],\n",
    "    n_head=model_config['n_head'],\n",
    "    n_kv_head=model_config['n_kv_head'],\n",
    "    n_embd=model_config['n_embd']\n",
    ")\n",
    "\n",
    "model = GPT(config)\n",
    "\n",
    "# Load weights\n",
    "print(\"Loading model weights...\")\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"âœ“ Model loaded: {num_params/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from nanochat.tokenizer import Tokenizer\n",
    "\n",
    "# Copy tokenizer files to expected location\n",
    "os.makedirs(os.path.expanduser('~/.cache/nanochat/tokenizer'), exist_ok=True)\n",
    "import shutil\n",
    "shutil.copy(tokenizer_path, os.path.expanduser('~/.cache/nanochat/tokenizer/tokenizer.pkl'))\n",
    "shutil.copy(token_bytes_path, os.path.expanduser('~/.cache/nanochat/tokenizer/token_bytes.pt'))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "print(f\"âœ“ Tokenizer loaded with vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 100,\n",
    "    temperature: float = 0.8,\n",
    "    top_k: int = 50,\n",
    "    direction: str = \"forward\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt.\n",
    "    \n",
    "    For backward models:\n",
    "    - Input prompt is the \"ending\" of the text\n",
    "    - Model generates what came \"before\"\n",
    "    - Output is reversed for display\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    # For backward models, reverse the input tokens\n",
    "    if direction == \"backward\":\n",
    "        tokens = tokens[::-1]\n",
    "    \n",
    "    # For bidirectional, add direction token\n",
    "    if direction == \"bidirectional\":\n",
    "        # Use forward direction by default for bidirectional\n",
    "        forward_token = tokenizer.special_tokens.get('<|forward|>')\n",
    "        if forward_token:\n",
    "            tokens = [forward_token] + tokens\n",
    "    \n",
    "    # Add BOS token\n",
    "    bos_token = tokenizer.special_tokens.get('<|begin_of_text|>', 0)\n",
    "    tokens = [bos_token] + tokens\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Generate\n",
    "    generated_tokens = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Get logits\n",
    "        logits = model(x)[0, -1, :]  # Last position logits\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Top-k sampling\n",
    "        if top_k > 0:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[-1]] = float('-inf')\n",
    "        \n",
    "        # Sample\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Check for EOS\n",
    "        eos_token = tokenizer.special_tokens.get('<|end_of_text|>')\n",
    "        if eos_token and next_token.item() == eos_token:\n",
    "            break\n",
    "        \n",
    "        generated_tokens.append(next_token.item())\n",
    "        x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if x.size(1) > config.sequence_len:\n",
    "            x = x[:, -config.sequence_len:]\n",
    "    \n",
    "    # Decode generated tokens\n",
    "    if direction == \"backward\":\n",
    "        # Reverse the generated tokens for display\n",
    "        generated_tokens = generated_tokens[::-1]\n",
    "        generated_text = tokenizer.decode(generated_tokens)\n",
    "        return generated_text + prompt  # Generated text comes before prompt\n",
    "    else:\n",
    "        generated_text = tokenizer.decode(generated_tokens)\n",
    "        return prompt + generated_text\n",
    "\n",
    "print(\"âœ“ Generation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try It Out!\n",
    "\n",
    "### Forward Generation (Standard)\n",
    "Give a beginning, model generates what comes next.\n",
    "\n",
    "### Backward Generation\n",
    "Give an ending, model generates what came before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate Text\n",
    "prompt = \"The quick brown fox\" #@param {type:\"string\"}\n",
    "max_tokens = 50 #@param {type:\"slider\", min:10, max:200, step:10}\n",
    "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:2.0, step:0.1}\n",
    "top_k = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "\n",
    "print(f\"Direction: {direction}\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "output = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    direction=direction\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated text:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Backward Model Usage\n",
    "\n",
    "With a backward model, you provide the *ending* and the model generates what *came before*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if direction == \"backward\":\n",
    "    # Example: Give an ending, get the beginning\n",
    "    ending = \"and they lived happily ever after.\"\n",
    "    \n",
    "    print(f\"Ending (your input): '{ending}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    story = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=ending,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        direction=direction\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nGenerated story (beginning + your ending):\\n{story}\")\n",
    "else:\n",
    "    print(f\"Current model is {direction}, not backward.\")\n",
    "    print(\"Select 'nanochat-760M-backward' or 'nanochat-760M-backward-sft' to try backward generation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Chat (for SFT model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_message: str, max_tokens: int = 150, temperature: float = 0.8):\n",
    "    \"\"\"\n",
    "    Simple chat interface for the SFT model.\n",
    "    \"\"\"\n",
    "    # Format as chat\n",
    "    prompt = f\"User: {user_message}\\nAssistant:\"\n",
    "    \n",
    "    output = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=50,\n",
    "        direction=direction\n",
    "    )\n",
    "    \n",
    "    # Extract assistant response\n",
    "    if \"Assistant:\" in output:\n",
    "        response = output.split(\"Assistant:\")[-1].strip()\n",
    "        # Stop at next \"User:\" if present\n",
    "        if \"User:\" in response:\n",
    "            response = response.split(\"User:\")[0].strip()\n",
    "        return response\n",
    "    return output\n",
    "\n",
    "if \"sft\" in MODEL_CHOICE:\n",
    "    print(\"Chat interface ready! Use the chat() function.\")\n",
    "    print(\"Example: chat('What is the capital of France?')\")\n",
    "else:\n",
    "    print(f\"Note: Current model ({MODEL_CHOICE}) is a base model, not SFT.\")\n",
    "    print(\"For better chat, select 'nanochat-760M-backward-sft'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Chat with the model\n",
    "user_input = \"Tell me a short story about a robot.\" #@param {type:\"string\"}\n",
    "\n",
    "if \"sft\" in MODEL_CHOICE:\n",
    "    response = chat(user_input)\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"\\nAssistant: {response}\")\n",
    "else:\n",
    "    # For base models, just do completion\n",
    "    output = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=user_input,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        direction=direction\n",
    "    )\n",
    "    print(f\"Completion:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About These Models\n",
    "\n",
    "These models are part of a research project studying how LLMs learn when trained in different directions:\n",
    "\n",
    "- **Backward models** predict tokens right-to-left instead of left-to-right\n",
    "- **Bidirectional models** can switch between both directions using special tokens\n",
    "\n",
    "### Research Questions:\n",
    "1. Do backward models learn different internal representations?\n",
    "2. Can models transfer knowledge across directions?\n",
    "3. Does bidirectional training help both directions?\n",
    "\n",
    "### Links:\n",
    "- **Code:** [github.com/traghav/onanchat](https://github.com/traghav/onanchat)\n",
    "- **Models:** [huggingface.co/raghavt](https://huggingface.co/raghavt)\n",
    "\n",
    "Based on [nanochat](https://github.com/karpathy/nanochat) by Andrej Karpathy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
