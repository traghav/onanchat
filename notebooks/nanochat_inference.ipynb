{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi3_ZuwdBXHA"
      },
      "source": [
        "# üîÑ nanochat Backward/Bidirectional Language Models\n",
        "\n",
        "This notebook lets you run inference with nanochat models trained in different directions:\n",
        "\n",
        "| Model | Direction | Description |\n",
        "|-------|-----------|-------------|\n",
        "| `nanochat-760M-backward` | Backward | Right-to-left prediction |\n",
        "| `nanochat-760M-bidirectional` | Bidirectional | Both directions with special tokens |\n",
        "| `nanochat-760M-backward-sft` | Backward | Chat fine-tuned backward model |\n",
        "\n",
        "**Repository:** [github.com/traghav/onanchat](https://github.com/traghav/onanchat)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrXVzNYUBXHB"
      },
      "source": [
        "## 1. Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M16AqDF5BXHB",
        "outputId": "7870a384-1cc0-4ebb-c2cd-cadc68113fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch huggingface_hub tiktoken maturin\n",
        "\n",
        "# Note: rustbpe is a custom Rust tokenizer that needs to be built\n",
        "# We'll build it after cloning the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvD9pGL0BXHB",
        "outputId": "bf1865d6-6896-4f6d-9ea6-322678e0452f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'onanchat'...\n",
            "remote: Enumerating objects: 897, done.\u001b[K\n",
            "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 897 (delta 75), reused 56 (delta 33), pack-reused 778 (from 3)\u001b[K\n",
            "Receiving objects: 100% (897/897), 568.84 KiB | 9.98 MiB/s, done.\n",
            "Resolving deltas: 100% (547/547), done.\n",
            "/content/onanchat/onanchat\n",
            "\u001b[1minfo:\u001b[0m downloading installer\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mprofile set to 'default'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault host triple is x86_64-unknown-linux-gnu\n",
            "\u001b[0m\u001b[1minfo: \u001b[0msyncing channel updates for 'stable-x86_64-unknown-linux-gnu'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mlatest update on 2025-11-10, rust version 1.91.1 (ed61e7d7e 2025-11-07)\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-docs'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rust-std'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustc'\n",
            " 74.5 MiB /  74.5 MiB (100 %)  55.6 MiB/s in  1s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdownloading component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'cargo'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'clippy'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-docs'\n",
            " 20.4 MiB /  20.4 MiB (100 %)   2.2 MiB/s in  7s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rust-std'\n",
            " 27.9 MiB /  27.9 MiB (100 %)  10.4 MiB/s in  3s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustc'\n",
            " 74.5 MiB /  74.5 MiB (100 %)  11.2 MiB/s in  6s\n",
            "\u001b[0m\u001b[1minfo: \u001b[0minstalling component 'rustfmt'\n",
            "\u001b[0m\u001b[1minfo: \u001b[0mdefault toolchain set to 'stable-x86_64-unknown-linux-gnu'\n",
            "\n",
            "  \u001b[0m\u001b[1m\u001b[0m\u001b[1m\u001b[32mstable-x86_64-unknown-linux-gnu installed\u001b[0m - rustc 1.91.1 (ed61e7d7e 2025-11-07)\n",
            "\n",
            "\u001b[0m\u001b[1m\n",
            "Rust is installed now. Great!\n",
            "\u001b[0m\n",
            "To get started you may need to restart your current shell.\n",
            "This would reload your \u001b[0m\u001b[1mPATH\u001b[0m environment variable to include\n",
            "Cargo's bin directory ($HOME/.cargo/bin).\n",
            "\n",
            "To configure your current shell, you need to source\n",
            "the corresponding \u001b[0m\u001b[1menv\u001b[0m file under $HOME/.cargo.\n",
            "\n",
            "This is usually done by running one of the following (note the leading DOT):\n",
            ". \"$HOME/.cargo/env\"            # For sh/bash/zsh/ash/dash/pdksh\n",
            "source \"$HOME/.cargo/env.fish\"  # For fish\n",
            "source $\"($nu.home-path)/.cargo/env.nu\"  # For nushell\n",
            "/content/onanchat/onanchat/rustbpe\n",
            "üí• maturin failed\n",
            "  Caused by: Couldn't find a virtualenv or conda environment, but you need one to use this command. For maturin to find your virtualenv you need to either set VIRTUAL_ENV (through activate), set CONDA_PREFIX (through conda activate) or have a virtualenv called .venv in the current or any parent folder. See https://virtualenv.pypa.io/en/latest/index.html on how to use virtualenv or use `maturin build` and `pip install <path/to/wheel>` instead.\n",
            "/content/onanchat/onanchat\n"
          ]
        }
      ],
      "source": [
        "# Clone the onanchat repository for model code\n",
        "!git clone https://github.com/traghav/onanchat.git\n",
        "%cd onanchat\n",
        "\n",
        "# Build the rustbpe tokenizer (requires Rust)\n",
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
        "import os\n",
        "os.environ[\"PATH\"] = f\"/root/.cargo/bin:{os.environ['PATH']}\"\n",
        "\n",
        "# Build rustbpe as a Python module\n",
        "%cd rustbpe\n",
        "!maturin develop --release\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht-eGotXBXHB",
        "outputId": "1275c0fa-defd-4b17-fb91-2da4e998be3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Add to path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qspFFmqGBXHC"
      },
      "source": [
        "## 2. Select and Download Model\n",
        "\n",
        "Choose which model to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuALii5hBXHC",
        "outputId": "e17183aa-4b66-4f9b-955c-5e0fe4b48957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: raghavt/nanochat-760M-backward\n"
          ]
        }
      ],
      "source": [
        "#@title Select Model\n",
        "MODEL_CHOICE = \"nanochat-760M-backward\" #@param [\"nanochat-760M-backward\", \"nanochat-760M-bidirectional\", \"nanochat-760M-backward-sft\"]\n",
        "\n",
        "REPO_ID = f\"raghavt/{MODEL_CHOICE}\"\n",
        "print(f\"Selected model: {REPO_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KwdnUZfBXHC",
        "outputId": "5f16a5f8-5074-4885-db88-b613b5dc5b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading model files...\n",
            "‚úì Model downloaded to: /root/.cache/huggingface/hub/models--raghavt--nanochat-760M-backward/snapshots/ff53bd7dfc098077b8cc4c81b6ab79d4a257f192/model.pt\n",
            "‚úì Metadata downloaded to: /root/.cache/huggingface/hub/models--raghavt--nanochat-760M-backward/snapshots/ff53bd7dfc098077b8cc4c81b6ab79d4a257f192/meta.json\n",
            "‚úì Tokenizer downloaded to: /root/.cache/huggingface/hub/models--raghavt--nanochat-760M-backward/snapshots/ff53bd7dfc098077b8cc4c81b6ab79d4a257f192/tokenizer.pkl\n"
          ]
        }
      ],
      "source": [
        "# Download model files from HuggingFace\n",
        "print(\"Downloading model files...\")\n",
        "\n",
        "model_path = hf_hub_download(repo_id=REPO_ID, filename=\"model.pt\")\n",
        "meta_path = hf_hub_download(repo_id=REPO_ID, filename=\"meta.json\")\n",
        "tokenizer_path = hf_hub_download(repo_id=REPO_ID, filename=\"tokenizer.pkl\")\n",
        "token_bytes_path = hf_hub_download(repo_id=REPO_ID, filename=\"token_bytes.pt\")\n",
        "\n",
        "print(f\"‚úì Model downloaded to: {model_path}\")\n",
        "print(f\"‚úì Metadata downloaded to: {meta_path}\")\n",
        "print(f\"‚úì Tokenizer downloaded to: {tokenizer_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNSevykBXHC"
      },
      "source": [
        "## 3. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I8K7DuNBXHC",
        "outputId": "307819ba-29bc-42de-8b94-d2b5de8ad1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model direction: backward\n",
            "Model config: {\n",
            "  \"sequence_len\": 2048,\n",
            "  \"vocab_size\": 65536,\n",
            "  \"n_layer\": 20,\n",
            "  \"n_head\": 10,\n",
            "  \"n_kv_head\": 10,\n",
            "  \"n_embd\": 1280\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from nanochat.gpt import GPT, GPTConfig\n",
        "\n",
        "# Load metadata\n",
        "with open(meta_path, 'r') as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "direction = meta.get('direction', 'forward')\n",
        "model_config = meta['model_config']\n",
        "\n",
        "print(f\"Model direction: {direction}\")\n",
        "print(f\"Model config: {json.dumps(model_config, indent=2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTdLzljWBXHC",
        "outputId": "1f3d77d7-5d08-4235-ad6b-f3054bd18917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model weights...\n",
            "‚úì Model loaded: 561.0M parameters\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "config = GPTConfig(\n",
        "    sequence_len=model_config['sequence_len'],\n",
        "    vocab_size=model_config['vocab_size'],\n",
        "    n_layer=model_config['n_layer'],\n",
        "    n_head=model_config['n_head'],\n",
        "    n_kv_head=model_config['n_kv_head'],\n",
        "    n_embd=model_config['n_embd']\n",
        ")\n",
        "\n",
        "model = GPT(config)\n",
        "\n",
        "# Load weights\n",
        "print(\"Loading model weights...\")\n",
        "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
        "model.load_state_dict(state_dict)\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úì Model loaded: {num_params/1e6:.1f}M parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f7T8j3HBXHC",
        "outputId": "7f98ca4e-ac49-4563-8405-4b4c872da617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Add to path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WShvaj28BXHC"
      },
      "source": [
        "## 4. Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvpnTMn8BXHC",
        "outputId": "0e7157af-a5f0-4025-c493-e8e883ed0388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Generation function defined\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_new_tokens: int = 100,\n",
        "    temperature: float = 0.8,\n",
        "    top_k: int = 50,\n",
        "    direction: str = \"forward\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate text from a prompt.\n",
        "\n",
        "    For backward models:\n",
        "    - Input prompt is the \"ending\" of the text\n",
        "    - Model generates what came \"before\"\n",
        "    - Output is reversed for display\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Encode prompt\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "\n",
        "    # For backward models, reverse the input tokens\n",
        "    if direction == \"backward\":\n",
        "        tokens = tokens[::-1]\n",
        "\n",
        "    # For bidirectional, add direction token\n",
        "    if direction == \"bidirectional\":\n",
        "        # Use forward direction by default for bidirectional\n",
        "        forward_token = tokenizer.get_forward_token_id()\n",
        "        tokens = [forward_token] + tokens\n",
        "\n",
        "    # Add BOS token\n",
        "    bos_token = tokenizer.get_bos_token_id()\n",
        "    tokens = [bos_token] + tokens\n",
        "\n",
        "    # Convert to tensor\n",
        "    x = torch.tensor([tokens], dtype=torch.long, device=device)\n",
        "\n",
        "    # Generate\n",
        "    generated_tokens = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Get logits\n",
        "        logits = model(x)[0, -1, :]  # Last position logits\n",
        "\n",
        "        # Apply temperature\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # Top-k sampling\n",
        "        if top_k > 0:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[-1]] = float('-inf')\n",
        "\n",
        "        # Sample\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        generated_tokens.append(next_token.item())\n",
        "        x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        # Truncate if too long\n",
        "        if x.size(1) > config.sequence_len:\n",
        "            x = x[:, -config.sequence_len:]\n",
        "\n",
        "    # Decode generated tokens\n",
        "    if direction == \"backward\":\n",
        "        # Reverse the generated tokens for display\n",
        "        generated_tokens = generated_tokens[::-1]\n",
        "        generated_text = tokenizer.decode(generated_tokens)\n",
        "        return generated_text + prompt  # Generated text comes before prompt\n",
        "    else:\n",
        "        generated_text = tokenizer.decode(generated_tokens)\n",
        "        return prompt + generated_text\n",
        "\n",
        "print(\"‚úì Generation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_OlUTuBXHD"
      },
      "source": [
        "## 5. Try It Out!\n",
        "\n",
        "### Forward Generation (Standard)\n",
        "Give a beginning, model generates what comes next.\n",
        "\n",
        "### Backward Generation\n",
        "Give an ending, model generates what came before!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vglyjtYFBXHD",
        "outputId": "b69d16ed-849c-4f56-8d15-8392cd9254c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Direction: backward\n",
            "Prompt: The quick brown fox\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated text:\n",
            " appropriate size for a medium-sized fox. The quick brown fox can also be referred to as the slow brown fox.\n",
            "What kind of animal is the slow brown fox called?\n",
            "The Quick Brown Fox\n",
            "What kind of animal is the quick brown fox called?\n",
            "The quick brown fox\n"
          ]
        }
      ],
      "source": [
        "#@title Generate Text\n",
        "prompt = \"The quick brown fox\" #@param {type:\"string\"}\n",
        "max_tokens = 50 #@param {type:\"slider\", min:10, max:200, step:10}\n",
        "temperature = 0.8 #@param {type:\"slider\", min:0.1, max:2.0, step:0.1}\n",
        "top_k = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "print(f\"Direction: {direction}\")\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "output = generate(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=max_tokens,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    direction=direction\n",
        ")\n",
        "\n",
        "print(f\"\\nGenerated text:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Skk7Y6BXHD"
      },
      "source": [
        "### Example: Backward Model Usage\n",
        "\n",
        "With a backward model, you provide the *ending* and the model generates what *came before*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LBi38HkBXHD",
        "outputId": "cb48c314-b05c-449f-eea0-9f9156e1880b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ending (your input): 'and they lived happily ever after.'\n",
            "--------------------------------------------------\n",
            "\n",
            "Generated story (beginning + your ending):\n",
            "which brought her love to him.\n",
            "it took very long to find the girl's love,\n",
            "which brought her love to him.\n",
            "it took very long to find the girl's love,\n",
            "which brought her love to him.\n",
            "it took very long to find that love,\n",
            "which brought his love to the girl, because after all,\n",
            "it took quite a while to find that love\n",
            "which brought her love to him.\n",
            "it took not very long to find that love,\n",
            "which brought the girl's love to him,\n",
            "and they lived happily ever after.\n"
          ]
        }
      ],
      "source": [
        "if direction == \"backward\":\n",
        "    # Example: Give an ending, get the beginning\n",
        "    ending = \"and they lived happily ever after.\"\n",
        "\n",
        "    print(f\"Ending (your input): '{ending}'\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    story = generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=ending,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.9,\n",
        "        top_k=50,\n",
        "        direction=direction\n",
        "    )\n",
        "\n",
        "    print(f\"\\nGenerated story (beginning + your ending):\\n{story}\")\n",
        "else:\n",
        "    print(f\"Current model is {direction}, not backward.\")\n",
        "    print(\"Select 'nanochat-760M-backward' or 'nanochat-760M-backward-sft' to try backward generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUlT9vWxBXHD"
      },
      "source": [
        "## 6. Interactive Chat (for SFT model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENVCw_v8BXHD",
        "outputId": "4ae5029b-83e8-4b29-d169-754f3f9034c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: Current model (nanochat-760M-backward) is a base model, not SFT.\n",
            "For better chat, select 'nanochat-760M-backward-sft'\n"
          ]
        }
      ],
      "source": [
        "def chat(user_message: str, max_tokens: int = 150, temperature: float = 0.8):\n",
        "    \"\"\"\n",
        "    Simple chat interface for the SFT model.\n",
        "    \"\"\"\n",
        "    # Format as chat\n",
        "    prompt = f\"User: {user_message}\\nAssistant:\"\n",
        "\n",
        "    output = generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_k=50,\n",
        "        direction=direction\n",
        "    )\n",
        "\n",
        "    # Extract assistant response\n",
        "    if \"Assistant:\" in output:\n",
        "        response = output.split(\"Assistant:\")[-1].strip()\n",
        "        # Stop at next \"User:\" if present\n",
        "        if \"User:\" in response:\n",
        "            response = response.split(\"User:\")[0].strip()\n",
        "        return response\n",
        "    return output\n",
        "\n",
        "if \"sft\" in MODEL_CHOICE:\n",
        "    print(\"Chat interface ready! Use the chat() function.\")\n",
        "    print(\"Example: chat('What is the capital of France?')\")\n",
        "else:\n",
        "    print(f\"Note: Current model ({MODEL_CHOICE}) is a base model, not SFT.\")\n",
        "    print(\"For better chat, select 'nanochat-760M-backward-sft'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yufJHrexBXHD",
        "outputId": "f9053015-da06-4a8c-9350-6e5ff3fcf9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion:\n",
            "ism.\n",
            "To learn more about what photorealism and photorealism mean, watch this video.\n",
            "Tell me a short story about hyperrealism.\n",
            "Realism is a one-dimensional way of looking at the world.\n",
            "Tell me a short story about hyperrealism.\n",
            "What is hyperrealism?\n",
            "We can view the world with hyperrealism.\n",
            "What is hyperrealism?\n",
            "Hyperrealism is a two-dimensional view of the world.\n",
            "What is a robot?\n",
            "Bring me a poem about a robot.\n",
            "Tell me a short story about a robot.\n"
          ]
        }
      ],
      "source": [
        "#@title Chat with the model\n",
        "user_input = \"Tell me a short story about a robot.\" #@param {type:\"string\"}\n",
        "\n",
        "if \"sft\" in MODEL_CHOICE:\n",
        "    response = chat(user_input)\n",
        "    print(f\"User: {user_input}\")\n",
        "    print(f\"\\nAssistant: {response}\")\n",
        "else:\n",
        "    # For base models, just do completion\n",
        "    output = generate(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=user_input,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        direction=direction\n",
        "    )\n",
        "    print(f\"Completion:\\n{output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqD45alpBXHD"
      },
      "source": [
        "---\n",
        "\n",
        "## About These Models\n",
        "\n",
        "These models are part of a research project studying how LLMs learn when trained in different directions:\n",
        "\n",
        "- **Backward models** predict tokens right-to-left instead of left-to-right\n",
        "- **Bidirectional models** can switch between both directions using special tokens\n",
        "\n",
        "### Research Questions:\n",
        "1. Do backward models learn different internal representations?\n",
        "2. Can models transfer knowledge across directions?\n",
        "3. Does bidirectional training help both directions?\n",
        "\n",
        "### Links:\n",
        "- **Code:** [github.com/traghav/onanchat](https://github.com/traghav/onanchat)\n",
        "- **Models:** [huggingface.co/raghavt](https://huggingface.co/raghavt)\n",
        "\n",
        "Based on [nanochat](https://github.com/karpathy/nanochat) by Andrej Karpathy."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}